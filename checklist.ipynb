{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386a347-ffca-4e46-a6fa-014c9515dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 아래페이지에서 순위가 1위부터 5위까지의 영화제목과 링크를 수집\n",
    "# https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cnt&date=20210525\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "url = \"https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cnt&date=20210520\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "arr = []\n",
    "for tmp in soup.select(\"div.tit3\", limit=5 ):\n",
    "    print(tmp.select_one('a').attrs['title'], 'https://movie.naver.com' + tmp.select_one('a').attrs['href'])    \n",
    "    \n",
    "    arr.append('https://movie.naver.com/' + tmp.select_one('a').attrs['href'])\n",
    "\n",
    "arr  # 수집된 영화상세페이지 링크 리스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467967ab-003c-4ebe-84e3-1868bf506a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 수집된 링크를 이용하여 상세페이지 수집(영화제목, 관람등급, 관객평점, 장르, 상영시간, 개봉일, 감독, 출연배우)\n",
    "# 이 부분은 위에서 수집한 링크 주소 예시1) https://movie.naver.com/movie/bi/mi/basic.nhn?code=189150\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "arr3 = []\n",
    "for tmp in arr:\n",
    "    url = tmp\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser') \n",
    "    \n",
    "    a = soup.select_one('#content > div.article > div.mv_info_area > div.mv_info > h3 > a:nth-child(1)')\n",
    "    print(\"영화제목 : \", a.text)\n",
    "\n",
    "    b = soup.select_one('#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(2) > p > span:nth-child(1) > a')\n",
    "    print(\"장르 : \", b.text)\n",
    "\n",
    "    c = soup.select_one(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(8) > p > a\")\n",
    "    print(\"관람등급 : \", c.text)\n",
    "    \n",
    "    d = soup.select_one(\"#actualPointPersentBasic > div > span > span\")\n",
    "    print(\"관람객평점 : \", d.text)\n",
    "\n",
    "    e = soup.select_one(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(2) > p > span:nth-child(3)\")\n",
    "    print(\"상영시간 : \", e.text)\n",
    "    \n",
    "    f = soup.select_one(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(2) > p > span:nth-child(4) > a:nth-child(1)\")\n",
    "    g = soup.select_one(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(2) > p > span:nth-child(4) > a:nth-child(2)\")\n",
    "    print(\"개봉일 : \" , f.text.strip() +  g.text)\n",
    "\n",
    "    h = soup.select_one(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(4)\")\n",
    "    print(\"감독 : \", h.text)\n",
    "\n",
    "    j = []\n",
    "    for tmp in soup.select(\"#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-child(6) > p\"):\n",
    "        j.append(tmp.text)\n",
    "        \n",
    "    print(\"배우 : \", ','.join(j)); \n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "        \n",
    "    arr3.append({\"a\":a.text, \"b\":b.text,\"c\":c.text,\"d\":d.text,\"e\":e.text,\"f\":f.text.strip() +  g.text, \"h\":h.text, \"j\": ','.join(j)})\n",
    "\n",
    "\n",
    "#----------------------\n",
    "arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dcb07-142c-40d6-b9d2-911146b38214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr4 = arr3  # 내용은 같으면서 객체도 같은\n",
    "\n",
    "import copy\n",
    "arr4 = copy.deepcopy(arr3)\n",
    "\n",
    "# arr3과 arr4는 내용은 같으면서 객체는 다른 객체\n",
    "\n",
    "# arr3 => mongo\n",
    "# arr4 => hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9593d9-ec2b-48d1-bf85-589d631e6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver  # pip install selenium\n",
    "import bs4 # pip install bs4\n",
    "import time\n",
    "import urllib.request as REQ  #서버 요청\n",
    "\n",
    "# 옵션설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"disable-gpu\")   \n",
    "options.add_argument(\"lang=ko_KR\")  \n",
    "# options.add_argument(\"headless\")  # 크롬창이 표시되지 않음\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36')  \n",
    "\n",
    "# 드라이브 로딩\n",
    "driver = webdriver.Chrome(executable_path=\"driver/chromedriver.exe\", options=options)\n",
    "\n",
    "for tmp in arr:\n",
    "    url = tmp\n",
    "    \n",
    "    # 페이지 접속\n",
    "    driver.get(url)\n",
    "    \n",
    "    # 로딩되는 시간이 있으므로 1초 기다림\n",
    "    time.sleep(1)\n",
    "\n",
    "    img = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]/div[2]/a/img')\n",
    "    print(img.get_attribute(\"src\"))\n",
    "    \n",
    "    title = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]/div[1]/h3/a[1]').text\n",
    "    \n",
    "    # 제목에 : 있으면 파일저장 : 공백으로 처리\n",
    "    title = title.replace(\":\", \"\")\n",
    "    \n",
    "    #서버요청(이미지의 url주소, 저장할 파일명)\n",
    "    REQ.urlretrieve( img.get_attribute(\"src\"), f'download/{title}.jpg' )\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36a723-7673-4d2a-8760-83b3cb84445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "try:\n",
    "    username = \"id100\"\n",
    "    password = \"pw100\"\n",
    "    host = \"1.234.5.158\"\n",
    "    port = 37017\n",
    "    dbname = \"id100\"\n",
    "    \n",
    "    conn = pymongo.MongoClient(f'mongodb://{username}:{password}@{host}:{port}/{dbname}')\n",
    "    db = conn.get_database(dbname)\n",
    "    collection = db.get_collection(\"exam_movie\")\n",
    "\n",
    "    # 여러개 추가하기\n",
    "    collection.insert_many(arr3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1dc3e-4df2-436b-9f8b-47c159a9cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdfs\n",
    "import json\n",
    "\n",
    "client_hdfs = hdfs.InsecureClient('http://1.234.5.158:9870', user='hdfs')\n",
    "\n",
    "client_hdfs.delete('/id100/exam_movie.json')\n",
    "with client_hdfs.write('/id100/exam_movie.json', encoding = \"utf-8\" ) as writer:\n",
    "    json.dump(arr4, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60044af5-c2b8-48c0-b572-34a3914291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with client_hdfs.read('/id100/exam_movie.json', encoding = 'utf-8') as reader:\n",
    "    df2 = pd.read_json(reader)\n",
    "    \n",
    "df2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
